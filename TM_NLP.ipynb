{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1748269423049,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "yhgYu18FM-_L"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "934uDa63sRnp"
   },
   "source": [
    "# Step 1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1748269423697,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "cxDjTTcDrtVl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8VrHduYC-kd"
   },
   "source": [
    "**This is how to identify which data set to use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4956,
     "status": "ok",
     "timestamp": 1748269428660,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "gvU6i2KNHzC4",
    "outputId": "86c7a472-d538-4cf4-b9a2-430eff047d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type your student number?310117329\n"
     ]
    }
   ],
   "source": [
    "index=input('type your student number?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748269428675,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "a8Gmf-HdGcWV",
    "outputId": "d8f1a62f-0603-4aaa-c738-2c82748a0d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your data set index ----> ( 1 0 )\n"
     ]
    }
   ],
   "source": [
    "x=divmod(int(index),4)\n",
    "yourdata1=x[1]\n",
    "y=divmod(int(index),3)\n",
    "yourdata2=y[1]\n",
    "\n",
    "print('This is your data set index ----> (', x[1], y[1], ')' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748269428680,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "HXPpVRSGAPM7",
    "outputId": "99d9fd60-d678-48da-f5b8-d7056f86b916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'alt.atheism']\n"
     ]
    }
   ],
   "source": [
    "data1= twenty_train.target_names[x[1]]\n",
    "data2= twenty_train.target_names[y[1]]\n",
    "categories1=[data1,data2]\n",
    "print(categories1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1jAHpjtaSPu"
   },
   "source": [
    "# Step 2 Process your text data, extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El_vU9NocxVC"
   },
   "source": [
    "# 2.1 An example of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1748269428870,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "wC3yT07PJnKp",
    "outputId": "0d79103d-59df-49fd-c8a2-2b49dac9c744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dotsonm@dmapub.dma.org (Mark Dotson)\n",
      "Subject: Re: Hell_2:  Black Sabbath\n",
      "Organization: Dayton Microcomputer Association; Dayton, Ohio\n",
      "Lines: 10\n",
      "\n",
      ": I may be wrong, but wasn't Jeff Fenholt part of Black Sabbath?  He's a\n",
      ": MAJOR brother in Christ now.  He totally changed his life around, and\n",
      ": he and his wife go on tours singing, witnessing, and spreading the\n",
      ": gospel for Christ.  I may be wrong about Black Sabbath, but I know he\n",
      ": was in a similar band if it wasn't that particular group...\n",
      "\n",
      "   Yes, but Jeff also speaks out against listening to bands like Black\n",
      "Sabbath. He says they're into all sorts of satanic stuff. I don't know.\n",
      "\n",
      "                          Mark (dotsonm@dmapub.dma.org)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your own NLP precessing examples with  preprocessing techniques.\n",
    "\n",
    "dataset=twenty_train.data[20]\n",
    "print(dataset)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748269428882,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "cRHBOWMnAAWA",
    "outputId": "858c3e54-2f9f-4d7d-e028-f4c84b191e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------tokenize:\n",
      "['From', ':', 'dotsonm', '@', 'dmapub.dma.org', '(', 'Mark', 'Dotson', ')', 'Subject', ':', 'Re', ':', 'Hell_2', ':', 'Black', 'Sabbath', 'Organization', ':', 'Dayton', 'Microcomputer', 'Association', ';', 'Dayton', ',', 'Ohio', 'Lines', ':', '10', ':', 'I', 'may', 'be', 'wrong', ',', 'but', 'was', \"n't\", 'Jeff', 'Fenholt', 'part', 'of', 'Black', 'Sabbath', '?', 'He', \"'s\", 'a', ':', 'MAJOR', 'brother', 'in', 'Christ', 'now', '.', 'He', 'totally', 'changed', 'his', 'life', 'around', ',', 'and', ':', 'he', 'and', 'his', 'wife', 'go', 'on', 'tours', 'singing', ',', 'witnessing', ',', 'and', 'spreading', 'the', ':', 'gospel', 'for', 'Christ', '.', 'I', 'may', 'be', 'wrong', 'about', 'Black', 'Sabbath', ',', 'but', 'I', 'know', 'he', ':', 'was', 'in', 'a', 'similar', 'band', 'if', 'it', 'was', \"n't\", 'that', 'particular', 'group', '...', 'Yes', ',', 'but', 'Jeff', 'also', 'speaks', 'out', 'against', 'listening', 'to', 'bands', 'like', 'Black', 'Sabbath', '.', 'He', 'says', 'they', \"'re\", 'into', 'all', 'sorts', 'of', 'satanic', 'stuff', '.', 'I', 'do', \"n't\", 'know', '.', 'Mark', '(', 'dotsonm', '@', 'dmapub.dma.org', ')']\n"
     ]
    }
   ],
   "source": [
    "# nltk tokenize\n",
    "example = \"This is an example sentence.\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_tokenize =word_tokenize(example)\n",
    "example_tokenize= word_tokenize(dataset) \n",
    "print(\"-------------------------tokenize:\")\n",
    "print(example_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1748269428890,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "kAaaP86_Ahmo",
    "outputId": "646ec18a-c3f7-4644-a91f-178f6666ddc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------stem:\n",
      "from: dotsonm@dmapub.dma.org (mark dotson)\n",
      "subject: re: hell_2:  black sabbath\n",
      "organization: dayton microcomputer association; dayton, ohio\n",
      "lines: 10\n",
      "\n",
      ": i may be wrong, but wasn't jeff fenholt part of black sabbath?  he's a\n",
      ": major brother in christ now.  he totally changed his life around, and\n",
      ": he and his wife go on tours singing, witnessing, and spreading the\n",
      ": gospel for christ.  i may be wrong about black sabbath, but i know he\n",
      ": was in a similar band if it wasn't that particular group...\n",
      "\n",
      "   yes, but jeff also speaks out against listening to bands like black\n",
      "sabbath. he says they're into all sorts of satanic stuff. i don't know.\n",
      "\n",
      "                          mark (dotsonm@dmapub.dma.org)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nltk stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "example_stem = stemmer.stem(dataset)\n",
    "print(\"-------------------------stem:\")\n",
    "print(example_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1748269428947,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "iL4Vr-m0ApLu",
    "outputId": "0e5d53d6-8aee-440f-9c49-360366016173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------pos_taging:\n",
      "[('From', 'IN'), (':', ':'), ('dotsonm', 'NN'), ('@', 'NN'), ('dmapub.dma.org', 'NN'), ('(', '('), ('Mark', 'NNP'), ('Dotson', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('Re', 'NN'), (':', ':'), ('Hell_2', 'NN'), (':', ':'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('Organization', 'NNP'), (':', ':'), ('Dayton', 'NNP'), ('Microcomputer', 'NNP'), ('Association', 'NNP'), (';', ':'), ('Dayton', 'NNP'), (',', ','), ('Ohio', 'NNP'), ('Lines', 'NNPS'), (':', ':'), ('10', 'CD'), (':', ':'), ('I', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('wrong', 'JJ'), (',', ','), ('but', 'CC'), ('was', 'VBD'), (\"n't\", 'RB'), ('Jeff', 'NNP'), ('Fenholt', 'NNP'), ('part', 'NN'), ('of', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('?', '.'), ('He', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), (':', ':'), ('MAJOR', 'NN'), ('brother', 'NN'), ('in', 'IN'), ('Christ', 'NNP'), ('now', 'RB'), ('.', '.'), ('He', 'PRP'), ('totally', 'RB'), ('changed', 'VBD'), ('his', 'PRP$'), ('life', 'NN'), ('around', 'RB'), (',', ','), ('and', 'CC'), (':', ':'), ('he', 'PRP'), ('and', 'CC'), ('his', 'PRP$'), ('wife', 'NN'), ('go', 'VBP'), ('on', 'IN'), ('tours', 'NNS'), ('singing', 'VBG'), (',', ','), ('witnessing', 'VBG'), (',', ','), ('and', 'CC'), ('spreading', 'VBG'), ('the', 'DT'), (':', ':'), ('gospel', 'NN'), ('for', 'IN'), ('Christ', 'NNP'), ('.', '.'), ('I', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('wrong', 'JJ'), ('about', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('know', 'VBP'), ('he', 'PRP'), (':', ':'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('similar', 'JJ'), ('band', 'NN'), ('if', 'IN'), ('it', 'PRP'), ('was', 'VBD'), (\"n't\", 'RB'), ('that', 'IN'), ('particular', 'JJ'), ('group', 'NN'), ('...', ':'), ('Yes', 'UH'), (',', ','), ('but', 'CC'), ('Jeff', 'NNP'), ('also', 'RB'), ('speaks', 'VBZ'), ('out', 'RP'), ('against', 'IN'), ('listening', 'VBG'), ('to', 'TO'), ('bands', 'NNS'), ('like', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('.', '.'), ('He', 'PRP'), ('says', 'VBZ'), ('they', 'PRP'), (\"'re\", 'VBP'), ('into', 'IN'), ('all', 'DT'), ('sorts', 'NNS'), ('of', 'IN'), ('satanic', 'JJ'), ('stuff', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('.', '.'), ('Mark', 'NNP'), ('(', '('), ('dotsonm', 'JJ'), ('@', 'NNP'), ('dmapub.dma.org', 'NN'), (')', ')')]\n"
     ]
    }
   ],
   "source": [
    "# nltk pos tagging\n",
    "example_posTag=nltk.pos_tag(example_tokenize)\n",
    "print(\"-------------------------pos_taging:\")\n",
    "print(example_posTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1748269428953,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "O-n6AAbc_-_5",
    "outputId": "282dde99-5399-4fe9-e901-c76d205ad4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  From/IN\n",
      "  :/:\n",
      "  (NP dotsonm/NN)\n",
      "  (NP @/NN)\n",
      "  (NP dmapub.dma.org/NN)\n",
      "  (/(\n",
      "  Mark/NNP\n",
      "  Dotson/NNP\n",
      "  )/)\n",
      "  (NP Subject/NN)\n",
      "  :/:\n",
      "  (NP Re/NN)\n",
      "  :/:\n",
      "  (NP Hell_2/NN)\n",
      "  :/:\n",
      "  Black/NNP\n",
      "  Sabbath/NNP\n",
      "  Organization/NNP\n",
      "  :/:\n",
      "  Dayton/NNP\n",
      "  Microcomputer/NNP\n",
      "  Association/NNP\n",
      "  ;/:\n",
      "  Dayton/NNP\n",
      "  ,/,\n",
      "  Ohio/NNP\n",
      "  Lines/NNPS\n",
      "  :/:\n",
      "  10/CD\n",
      "  :/:\n",
      "  I/PRP\n",
      "  may/MD\n",
      "  be/VB\n",
      "  wrong/JJ\n",
      "  ,/,\n",
      "  but/CC\n",
      "  was/VBD\n",
      "  n't/RB\n",
      "  Jeff/NNP\n",
      "  Fenholt/NNP\n",
      "  (NP part/NN)\n",
      "  of/IN\n",
      "  Black/NNP\n",
      "  Sabbath/NNP\n",
      "  ?/.\n",
      "  He/PRP\n",
      "  's/VBZ\n",
      "  a/DT\n",
      "  :/:\n",
      "  (NP MAJOR/NN)\n",
      "  (NP brother/NN)\n",
      "  in/IN\n",
      "  Christ/NNP\n",
      "  now/RB\n",
      "  ./.\n",
      "  He/PRP\n",
      "  totally/RB\n",
      "  changed/VBD\n",
      "  his/PRP$\n",
      "  (NP life/NN)\n",
      "  around/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  :/:\n",
      "  he/PRP\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  (NP wife/NN)\n",
      "  go/VBP\n",
      "  on/IN\n",
      "  tours/NNS\n",
      "  singing/VBG\n",
      "  ,/,\n",
      "  witnessing/VBG\n",
      "  ,/,\n",
      "  and/CC\n",
      "  spreading/VBG\n",
      "  the/DT\n",
      "  :/:\n",
      "  (NP gospel/NN)\n",
      "  for/IN\n",
      "  Christ/NNP\n",
      "  ./.\n",
      "  I/PRP\n",
      "  may/MD\n",
      "  be/VB\n",
      "  wrong/JJ\n",
      "  about/IN\n",
      "  Black/NNP\n",
      "  Sabbath/NNP\n",
      "  ,/,\n",
      "  but/CC\n",
      "  I/PRP\n",
      "  know/VBP\n",
      "  he/PRP\n",
      "  :/:\n",
      "  was/VBD\n",
      "  in/IN\n",
      "  (NP a/DT similar/JJ band/NN)\n",
      "  if/IN\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  n't/RB\n",
      "  that/IN\n",
      "  (NP particular/JJ group/NN)\n",
      "  .../:\n",
      "  Yes/UH\n",
      "  ,/,\n",
      "  but/CC\n",
      "  Jeff/NNP\n",
      "  also/RB\n",
      "  speaks/VBZ\n",
      "  out/RP\n",
      "  against/IN\n",
      "  listening/VBG\n",
      "  to/TO\n",
      "  bands/NNS\n",
      "  like/IN\n",
      "  Black/NNP\n",
      "  Sabbath/NNP\n",
      "  ./.\n",
      "  He/PRP\n",
      "  says/VBZ\n",
      "  they/PRP\n",
      "  're/VBP\n",
      "  into/IN\n",
      "  all/DT\n",
      "  sorts/NNS\n",
      "  of/IN\n",
      "  (NP satanic/JJ stuff/NN)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  know/VB\n",
      "  ./.\n",
      "  Mark/NNP\n",
      "  (/(\n",
      "  dotsonm/JJ\n",
      "  @/NNP\n",
      "  (NP dmapub.dma.org/NN)\n",
      "  )/))\n"
     ]
    }
   ],
   "source": [
    " # consituency parsing, chunking\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(example_posTag)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7VpnVNpKuUt"
   },
   "source": [
    "#2.2 NLP Preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1748269428973,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "B8u5y9adK3tc",
    "outputId": "e0b28a2a-fbe4-4891-d0d2-88d9259d4565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "stopwordEn = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def lemmaWord(word):\n",
    "    lemma = wordnet.morphy(word)\n",
    "    if lemma is not None:\n",
    "        return lemma\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def stemWord(word):\n",
    "    stem = stemmer.stem(word)\n",
    "    if stem is not None:\n",
    "        return stem\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Base preprocessing method\n",
    "def processText(text, lemma=False, stem=False, gram=1, rmStop=True): # default no lemma or stem, unigram and remove stop words\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE)\n",
    "    # The regex @\\w+ matches an @ followed by word characters, not for full email addresses\n",
    "    tokens = word_tokenize(text)\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    new_tokens = []\n",
    "    stoplist = stopwordEn if rmStop else []\n",
    "    for i in tokens:\n",
    "        i = i.lower()\n",
    "        if i.isalpha() and (i not in stoplist or i in whitelist):\n",
    "            if lemma:\n",
    "                i = lemmaWord(i)\n",
    "            if stem:\n",
    "                i = stemWord(i)\n",
    "            new_tokens.append(i)\n",
    "    del tokens\n",
    "    if gram <= 1:\n",
    "        return new_tokens\n",
    "    else:\n",
    "        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n",
    "\n",
    "# Further preprocessing method to remove email headers and addresses\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'^(From|Subject|Organization|Lines|Reply-To|NNTP-Posting-Host):.*$', '', text, flags=re.MULTILINE) # Remove headers (From:, Subject:)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text) # Remove full email addresses\n",
    "    return text\n",
    "\n",
    "# Get POS tags method\n",
    "def getTags(text):\n",
    "  token = word_tokenize(text)\n",
    "  token = [l.lower() for l in token]\n",
    "  train_tags = nltk.pos_tag(token)\n",
    "  return [i[1] for i in train_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1748269428982,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "U8mwYOcFcS02",
    "outputId": "34423a12-6509-42ef-c166-d4c3c877ea9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mark', 'dotson', 'subject', 'black', 'sabbath', 'organization', 'dayton', 'microcomputer', 'association', 'dayton', 'ohio', 'lines', 'may', 'wrong', 'jeff', 'fenholt', 'part', 'black', 'sabbath', 'major', 'brother', 'christ', 'totally', 'changed', 'life', 'around', 'wife', 'go', 'tours', 'singing', 'witnessing', 'spreading', 'gospel', 'christ', 'may', 'wrong', 'black', 'sabbath', 'know', 'similar', 'band', 'particular', 'group', 'yes', 'jeff', 'also', 'speaks', 'listening', 'bands', 'like', 'black', 'sabbath', 'says', 'sorts', 'satanic', 'stuff', 'know', 'mark']\n"
     ]
    }
   ],
   "source": [
    "print(processText(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44xTvpLa_UC9"
   },
   "source": [
    "# Step 3: Build a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1748269429567,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "i9HMKvgGMHPB",
    "outputId": "36ccaf74-e897-4e9c-c202-82596fd9d93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " From: frank@D012S658.uucp (Frank O'Dwyer)\n",
      "Subject: Re: After 2000 years, can we say that Christian Morality is\n",
      "Organization: Siemens-Nixdorf AG\n",
      "Lines: 28\n",
      "NNTP-Posting-Host: d012s658.ap.mchp.sni.de\n",
      "\n",
      "In article <1993Apr15.125245.12872@abo.fi> MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka) writes:\n",
      "|In <1qie61$fkt@horus.ap.mchp.sni.de> frank@D012S658.uucp writes:\n",
      "|> In article <30114@ursa.bear.com> halat@pooh.bears (Jim Halat) writes:\n",
      "|\n",
      "|> #I'm one of those people who does not know what the word objectiv\n",
      "Cleaned text:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In article   (Mats Andtbacka) writes:\n",
      "|In   writes:\n",
      "|> In article   (Jim Halat) writes:\n",
      "|\n",
      "|> #I'm one of those people who does not know what the word objective means \n",
      "|> #when put next to the word morality.  I assume its an idiom and cannot\n",
      "|> #be defined by its separate terms.\n",
      "|> #\n",
      "|> #Give it a try.\n",
      "|> \n",
      "|> Objective morality is morality built from objective values.\n",
      "|\n",
      "|      \"And these objective values are ... ?\"\n",
      "|Please be specific, and more importantly, motivate.\n",
      "\n",
      "I'll take a wild guess\n"
     ]
    }
   ],
   "source": [
    "# Show before/after cleaning text data\n",
    "\n",
    "print(\"Original text:\\n\", fetch_20newsgroups(subset='train', categories=categories1).data[0][:500])\n",
    "print(\"Cleaned text:\\n\", cleanText(fetch_20newsgroups(subset='train', categories=categories1).data[0])[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1748269430369,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "9PDFkEEiL1GQ"
   },
   "outputs": [],
   "source": [
    "twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)\n",
    "twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "raw_train_data = twenty_train1.data  # RAW text data - NO cleaning\n",
    "raw_test_data = twenty_test1.data # RAW\n",
    "\n",
    "clean_train_data = [cleanText(doc) for doc in raw_train_data] # CLEANED text data\n",
    "clean_test_data = [cleanText(doc) for doc in raw_test_data] # CLEANED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1748269430432,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "qx9DEr9nY6sl",
    "outputId": "ccada15a-1bda-4a49-e10b-41583f2d1feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing:\n",
      " From: frank@D012S658.uucp (Frank O'Dwyer)\n",
      "Subject: Re: After 2000 years, can we say that Christian Morality is\n",
      "Organization: Siemens-Nixdorf AG\n",
      "Lines: 28\n",
      "NNTP-Posting-Host: d012s658.ap.mchp.sni.de\n",
      "\n",
      "In article <1993Apr15.125245.12872@abo.fi> MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka) writes:\n",
      "|In <1qie61$fkt@horus.ap.mchp.sni.de> frank@D012S658.uucp writes:\n",
      "|> In article <30114@ursa.bear.com> halat@pooh.bears (Jim Halat) writes:\n",
      "|\n",
      "|> #I'm one of those people who does not know what the word objective means \n",
      "|> #when put next to the word morality.  I assume its an idiom and cannot\n",
      "|> #be defined by its separate terms.\n",
      "|> #\n",
      "|> #Give it a try.\n",
      "|> \n",
      "|> Objective morality is morality built from objective values.\n",
      "|\n",
      "|      \"And these objective values are ... ?\"\n",
      "|Please be specific, and more importantly, motivate.\n",
      "\n",
      "I'll take a wild guess and say Freedom is objectively valuable.  I base\n",
      "this on the assumption that if everyone in the world were deprived utterly\n",
      "of their freedom (so that their every act was contrary to their volition),\n",
      "almost all would want to complain.  Therefore I take it that to assert or\n",
      "believe that \"Freedom is not very valuable\", when almost everyone can see\n",
      "that it is, is every bit as absurd as to assert \"it is not raining\" on\n",
      "a rainy day.  I take this to be a candidate for an objective value, and it\n",
      "it is a necessary condition for objective morality that objective values\n",
      "such as this exist.\n",
      "\n",
      "-- \n",
      "Frank O'Dwyer                                  'I'm not hatching That'\n",
      "odwyer@sse.ie                                  from \"Hens\",  by Evelyn Conlon\n",
      "\n",
      "After processing:\n",
      " ['frank', 'subject', 'years', 'say', 'christian', 'morality', 'organization', 'ag', 'lines', 'article', 'mats', 'andtbacka', 'writes', 'writes', 'article', 'jim', 'halat', 'writes', 'one', 'people', 'not', 'know', 'word', 'objective', 'means', 'put', 'next', 'word', 'morality', 'assume', 'idiom', 'not', 'defined', 'separate', 'terms', 'give', 'try', 'objective', 'morality', 'morality', 'built', 'objective', 'values', 'objective', 'values', 'specific', 'importantly', 'motivate', 'take', 'wild', 'guess', 'say', 'freedom', 'objectively', 'valuable', 'base', 'assumption', 'everyone', 'world', 'deprived', 'utterly', 'freedom', 'every', 'act', 'contrary', 'volition', 'almost', 'would', 'want', 'complain', 'therefore', 'take', 'assert', 'believe', 'freedom', 'not', 'valuable', 'almost', 'everyone', 'see', 'every', 'bit', 'absurd', 'assert', 'not', 'raining', 'rainy', 'day', 'take', 'candidate', 'objective', 'value', 'necessary', 'condition', 'objective', 'morality', 'objective', 'values', 'exist', 'frank', 'not', 'hatching', 'hens', 'evelyn', 'conlon']\n"
     ]
    }
   ],
   "source": [
    "sample_text = twenty_train1.data[0]\n",
    "\n",
    "# Before processing\n",
    "print(\"Before processing:\\n\", sample_text)\n",
    "\n",
    "# After processing\n",
    "print(\"After processing:\\n\", processText(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748269430433,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "-ZRqySa8cbr1",
    "outputId": "828c025e-1c61-4a76-9b08-6cd51e52d14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:\n",
      " ['IN', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'NN', ':', 'NN', ':', 'IN', 'CD', 'NNS', ',', 'MD', 'PRP', 'VB', 'IN', 'JJ', 'NN', 'VBZ', 'NN', ':', 'JJ', 'NN', 'NNS', ':', 'CD', 'NN', ':', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', '(', 'NNS', 'VBP', ')', 'VBZ', ':', 'NN', 'VBZ', 'CD', '$', 'JJ', 'NNP', 'NN', 'NNP', 'VBD', 'NNP', 'NN', 'NNS', ':', 'NN', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'JJ', 'NNP', 'NN', 'NN', 'NNS', '(', 'JJ', 'NN', ')', 'VBZ', ':', 'NN', 'NNP', 'NNP', '#', 'NN', 'VBP', 'CD', 'IN', 'DT', 'NNS', 'WP', 'VBZ', 'RB', 'VB', 'WP', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NNP', '#', 'WRB', 'VBN', 'RB', 'TO', 'DT', 'NN', 'NN', '.', 'JJ', 'VBP', 'PRP$', 'DT', 'NN', 'CC', 'MD', 'RB', 'VB', 'JJ', '#', 'VB', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', '.', 'NN', 'JJ', '#', 'NNP', 'NNP', '#', 'VB', 'PRP', 'DT', 'NN', '.', 'JJ', 'JJ', 'NN', 'NNP', 'JJ', 'NN', 'VBZ', 'JJ', 'VBN', 'IN', 'JJ', 'NNS', '.', 'VB', 'JJ', '``', 'CC', 'DT', 'JJ', 'NNS', 'VBP', ':', '.', \"''\", 'NN', 'VB', 'JJ', ',', 'CC', 'RBR', 'RB', ',', 'NN', '.', 'JJ', 'MD', 'VB', 'DT', 'JJ', 'NN', 'CC', 'VB', 'NN', 'VBZ', 'RB', 'JJ', '.', 'JJ', 'NN', 'DT', 'IN', 'DT', 'NN', 'IN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '(', 'IN', 'DT', 'PRP$', 'DT', 'NN', 'VBD', 'JJ', 'TO', 'PRP$', 'NN', ')', ',', 'RB', 'DT', 'MD', 'VB', 'TO', 'VB', '.', 'NN', 'JJ', 'VBP', 'PRP', 'IN', 'TO', 'VB', 'CC', 'VB', 'IN', '``', 'NN', 'VBZ', 'RB', 'RB', 'JJ', \"''\", ',', 'WRB', 'RB', 'NN', 'MD', 'VB', 'IN', 'PRP', 'VBZ', ',', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'IN', 'TO', 'VB', '``', 'PRP', 'VBZ', 'RB', 'VBG', \"''\", 'IN', 'DT', 'JJ', 'NN', '.', 'VB', 'VBP', 'DT', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'WDT', 'JJ', 'NNS', 'JJ', 'IN', 'DT', 'NN', '.', ':', 'JJ', 'NN', 'POS', 'NN', 'VBP', 'RB', 'VBG', 'IN', \"''\", 'JJ', 'NNP', 'NN', 'IN', '``', 'NNS', \"''\", ',', 'IN', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "print(\"POS tags:\\n\", getTags(twenty_train1.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748269430436,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "POlF_eC0QFuR",
    "outputId": "dc2beb12-c71f-4c80-8a8a-b07ba42ea8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frank', 'subject', 'years', 'say', 'christian', 'morality', 'organization', 'ag', 'lines', 'article', 'mats', 'andtbacka', 'writes', 'writes', 'article', 'jim', 'halat', 'writes', 'one', 'people', 'not', 'know', 'word', 'objective', 'means', 'put', 'next', 'word', 'morality', 'assume', 'idiom', 'not', 'defined', 'separate', 'terms', 'give', 'try', 'objective', 'morality', 'morality', 'built', 'objective', 'values', 'objective', 'values', 'specific', 'importantly', 'motivate', 'take', 'wild', 'guess', 'say', 'freedom', 'objectively', 'valuable', 'base', 'assumption', 'everyone', 'world', 'deprived', 'utterly', 'freedom', 'every', 'act', 'contrary', 'volition', 'almost', 'would', 'want', 'complain', 'therefore', 'take', 'assert', 'believe', 'freedom', 'not', 'valuable', 'almost', 'everyone', 'see', 'every', 'bit', 'absurd', 'assert', 'not', 'raining', 'rainy', 'day', 'take', 'candidate', 'objective', 'value', 'necessary', 'condition', 'objective', 'morality', 'objective', 'values', 'exist', 'frank', 'not', 'hatching', 'hens', 'evelyn', 'conlon']\n",
      "['IN', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'NN', ':', 'NN', ':', 'IN', 'CD', 'NNS', ',', 'MD', 'PRP', 'VB', 'IN', 'JJ', 'NN', 'VBZ', 'NN', ':', 'JJ', 'NN', 'NNS', ':', 'CD', 'NN', ':', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', '(', 'NNS', 'VBP', ')', 'VBZ', ':', 'NN', 'VBZ', 'CD', '$', 'JJ', 'NNP', 'NN', 'NNP', 'VBD', 'NNP', 'NN', 'NNS', ':', 'NN', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'JJ', 'NNP', 'NN', 'NN', 'NNS', '(', 'JJ', 'NN', ')', 'VBZ', ':', 'NN', 'NNP', 'NNP', '#', 'NN', 'VBP', 'CD', 'IN', 'DT', 'NNS', 'WP', 'VBZ', 'RB', 'VB', 'WP', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NNP', '#', 'WRB', 'VBN', 'RB', 'TO', 'DT', 'NN', 'NN', '.', 'JJ', 'VBP', 'PRP$', 'DT', 'NN', 'CC', 'MD', 'RB', 'VB', 'JJ', '#', 'VB', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', '.', 'NN', 'JJ', '#', 'NNP', 'NNP', '#', 'VB', 'PRP', 'DT', 'NN', '.', 'JJ', 'JJ', 'NN', 'NNP', 'JJ', 'NN', 'VBZ', 'JJ', 'VBN', 'IN', 'JJ', 'NNS', '.', 'VB', 'JJ', '``', 'CC', 'DT', 'JJ', 'NNS', 'VBP', ':', '.', \"''\", 'NN', 'VB', 'JJ', ',', 'CC', 'RBR', 'RB', ',', 'NN', '.', 'JJ', 'MD', 'VB', 'DT', 'JJ', 'NN', 'CC', 'VB', 'NN', 'VBZ', 'RB', 'JJ', '.', 'JJ', 'NN', 'DT', 'IN', 'DT', 'NN', 'IN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '(', 'IN', 'DT', 'PRP$', 'DT', 'NN', 'VBD', 'JJ', 'TO', 'PRP$', 'NN', ')', ',', 'RB', 'DT', 'MD', 'VB', 'TO', 'VB', '.', 'NN', 'JJ', 'VBP', 'PRP', 'IN', 'TO', 'VB', 'CC', 'VB', 'IN', '``', 'NN', 'VBZ', 'RB', 'RB', 'JJ', \"''\", ',', 'WRB', 'RB', 'NN', 'MD', 'VB', 'IN', 'PRP', 'VBZ', ',', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'IN', 'TO', 'VB', '``', 'PRP', 'VBZ', 'RB', 'VBG', \"''\", 'IN', 'DT', 'JJ', 'NN', '.', 'VB', 'VBP', 'DT', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'WDT', 'JJ', 'NNS', 'JJ', 'IN', 'DT', 'NN', '.', ':', 'JJ', 'NN', 'POS', 'NN', 'VBP', 'RB', 'VBG', 'IN', \"''\", 'JJ', 'NNP', 'NN', 'IN', '``', 'NNS', \"''\", ',', 'IN', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# Show tokens for document after processing\n",
    "print(processText(twenty_train1.data[0]))\n",
    "\n",
    "# Show POS tags for a document\n",
    "print(getTags(twenty_train1.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1748269441136,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "vNm3axlhdzlF"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "# Baseline - no cleaning, no lemma\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=processText)),  # processText with default lemma=False\n",
    "    ('tfidf', TfidfTransformer()), # TF-IDF Transformer\n",
    "    ('clf', LogisticRegression(max_iter=1000)) # Logistic Regression classifier with max iterations set to 1000 to reach convergence\n",
    "])\n",
    "\n",
    "\n",
    "# With cleanText, no lemma\n",
    "clean_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=processText)),  # processText with default lemma=False\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# With cleanText and lemma\n",
    "lemma_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=lambda x: processText(x, lemma=True))), # processText with lemma=True\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# With cleanText and stem\n",
    "stem_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=lambda x: processText(x, stem=True))), # processText with stem=True\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# With cleanText and lemma with stem\n",
    "lemmastem_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer=lambda x: processText(x, lemma=True, stem=True))),# processText with lemma=True AND stem=True\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 22916,
     "status": "ok",
     "timestamp": 1748269465952,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "Vuq37Bf3Qjpn",
    "outputId": "c176db6a-3d4b-4b98-a590-f18eb22699d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(analyzer=<function <lambda> at 0x7a8678eff380>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train the models\n",
    "\n",
    "baseline_pipeline.fit(raw_train_data, twenty_train1.target)\n",
    "clean_pipeline.fit(clean_train_data, twenty_train1.target)\n",
    "lemma_pipeline.fit(clean_train_data, twenty_train1.target)\n",
    "stem_pipeline.fit(clean_train_data, twenty_train1.target)\n",
    "lemmastem_pipeline.fit(clean_train_data, twenty_train1.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjQ8DmPNRUuJ"
   },
   "source": [
    "# Step 4: Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 16503,
     "status": "ok",
     "timestamp": 1748269484418,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "OMdoIHjMRWce"
   },
   "outputs": [],
   "source": [
    "# To make predictions with dev/test set\n",
    "\n",
    "baseline_pred = baseline_pipeline.predict(raw_test_data)\n",
    "clean_pred = clean_pipeline.predict(clean_test_data)\n",
    "lemma_pred = lemma_pipeline.predict(clean_test_data)\n",
    "stem_pred = stem_pipeline.predict(clean_test_data)\n",
    "lemmastem_pred = lemmastem_pipeline.predict(clean_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GXHJHqoBmyJ"
   },
   "source": [
    "# Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1748269647651,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "LdB9js0QDErf",
    "outputId": "52e2e031-ee93-4557-c6d8-8210739c4144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline:\n",
      "Accuracy: 0.9505649717514124\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.91      0.94       319\n",
      "comp.graphics       0.93      0.99      0.96       389\n",
      "\n",
      "     accuracy                           0.95       708\n",
      "    macro avg       0.96      0.95      0.95       708\n",
      " weighted avg       0.95      0.95      0.95       708\n",
      "\n",
      "\n",
      "With Clean Text:\n",
      "Accuracy: 0.9519774011299436\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.97      0.92      0.95       319\n",
      "comp.graphics       0.94      0.98      0.96       389\n",
      "\n",
      "     accuracy                           0.95       708\n",
      "    macro avg       0.95      0.95      0.95       708\n",
      " weighted avg       0.95      0.95      0.95       708\n",
      "\n",
      "\n",
      "With Clean Text + Lemma:\n",
      "Accuracy: 0.9562146892655368\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.92      0.95       319\n",
      "comp.graphics       0.94      0.98      0.96       389\n",
      "\n",
      "     accuracy                           0.96       708\n",
      "    macro avg       0.96      0.95      0.96       708\n",
      " weighted avg       0.96      0.96      0.96       708\n",
      "\n",
      "\n",
      "With Clean Text + Stem:\n",
      "Accuracy: 0.96045197740113\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.93      0.95       319\n",
      "comp.graphics       0.94      0.99      0.96       389\n",
      "\n",
      "     accuracy                           0.96       708\n",
      "    macro avg       0.96      0.96      0.96       708\n",
      " weighted avg       0.96      0.96      0.96       708\n",
      "\n",
      "\n",
      "With Clean Text + Lemma + Stem:\n",
      "Accuracy: 0.96045197740113\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.93      0.95       319\n",
      "comp.graphics       0.94      0.99      0.96       389\n",
      "\n",
      "     accuracy                           0.96       708\n",
      "    macro avg       0.96      0.96      0.96       708\n",
      " weighted avg       0.96      0.96      0.96       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix # get evaluation metrics / error confusion matrix\n",
    "import pandas as pd\n",
    "\n",
    "# get all pipelines\n",
    "pipelines = [\n",
    "    (\"Baseline\", baseline_pred),\n",
    "    (\"With Clean Text\", clean_pred),\n",
    "    (\"With Clean Text + Lemma\", lemma_pred),\n",
    "    (\"With Clean Text + Stem\", stem_pred),\n",
    "    (\"With Clean Text + Lemma + Stem\", lemmastem_pred)\n",
    "]\n",
    "\n",
    "# for each pipeline, perform evaluation\n",
    "for name, preds in pipelines:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"Accuracy:\", accuracy_score(twenty_test1.target, preds))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(twenty_test1.target, preds, target_names=twenty_test1.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCLCqFXPQsRq"
   },
   "source": [
    "# Step 6: Error Analysis and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1748270229069,
     "user": {
      "displayName": "kris",
      "userId": "01768602552210981653"
     },
     "user_tz": -60
    },
    "id": "kvBw9qkKDS-m",
    "outputId": "4859aeee-2b1b-4211-e878-c76fc8f23297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassified examples for Baseline:\n",
      "                                                 news  prediction  true\n",
      "0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n",
      "4   Organization: Penn State University\\nFrom: <SM...           1     0\n",
      "15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n",
      "20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n",
      "82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n",
      "Confusion Matrix:\n",
      "               alt.atheism  comp.graphics\n",
      "alt.atheism            289             30\n",
      "comp.graphics            5            384\n",
      "\n",
      "Misclassified examples for With Clean Text:\n",
      "                                                 news  prediction  true\n",
      "0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n",
      "4   Organization: Penn State University\\nFrom: <SM...           1     0\n",
      "15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n",
      "20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n",
      "82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n",
      "Confusion Matrix:\n",
      "               alt.atheism  comp.graphics\n",
      "alt.atheism            293             26\n",
      "comp.graphics            8            381\n",
      "\n",
      "Misclassified examples for With Clean Text + Lemma:\n",
      "                                                 news  prediction  true\n",
      "0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n",
      "4   Organization: Penn State University\\nFrom: <SM...           1     0\n",
      "15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n",
      "20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n",
      "82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n",
      "Confusion Matrix:\n",
      "               alt.atheism  comp.graphics\n",
      "alt.atheism            295             24\n",
      "comp.graphics            7            382\n",
      "\n",
      "Misclassified examples for With Clean Text + Stem:\n",
      "                                                 news  prediction  true\n",
      "0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n",
      "4   Organization: Penn State University\\nFrom: <SM...           1     0\n",
      "15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n",
      "20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n",
      "82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n",
      "Confusion Matrix:\n",
      "               alt.atheism  comp.graphics\n",
      "alt.atheism            296             23\n",
      "comp.graphics            5            384\n",
      "\n",
      "Misclassified examples for With Clean Text + Lemma + Stem:\n",
      "                                                 news  prediction  true\n",
      "0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n",
      "4   Organization: Penn State University\\nFrom: <SM...           1     0\n",
      "15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n",
      "20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n",
      "82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n",
      "Confusion Matrix:\n",
      "               alt.atheism  comp.graphics\n",
      "alt.atheism            296             23\n",
      "comp.graphics            5            384\n"
     ]
    }
   ],
   "source": [
    "# for each pipeline, get misclassified examples (5) and confusion matrix\n",
    "for name, preds in pipelines:\n",
    "    df_pred = pd.DataFrame({\n",
    "        'news': twenty_test1.data,\n",
    "        'prediction': preds,\n",
    "        'true': twenty_test1.target\n",
    "    })\n",
    "\n",
    "    misclassified = df_pred[df_pred['true'] != df_pred['prediction']]\n",
    "    print(f\"\\nMisclassified examples for {name}:\")\n",
    "    print(misclassified.head(5))  # Only first 5 to show all clearly\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(pd.DataFrame(\n",
    "        confusion_matrix(twenty_test1.target, preds),\n",
    "        columns=twenty_test1.target_names,\n",
    "        index=twenty_test1.target_names\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRIRg2rufjPu"
   },
   "source": [
    "#References:  \n",
    "\n",
    "\n",
    "https://www.nltk.org/book/ch06.html\n",
    "\n",
    " https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "934uDa63sRnp",
    "El_vU9NocxVC"
   ],
   "provenance": [
    {
     "file_id": "1CXj_XvX-XiTFLsBe1_4V-M3vQYIPc_N8",
     "timestamp": 1743189787592
    },
    {
     "file_id": "1P-rVbBIU8McBJwvuWbMJjmRgaYLCZG48",
     "timestamp": 1634806079763
    },
    {
     "file_id": "15ZvXDbGU32-d3afddksL6uCbAarx-gl3",
     "timestamp": 1634806036046
    },
    {
     "file_id": "1I3g5DeuhS0sSOBd4DeQhR7OwjaSSx6gy",
     "timestamp": 1615032664939
    },
    {
     "file_id": "1lTmqMvgIU4LIna3kklDa8J7UncnycGCy",
     "timestamp": 1615030669644
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
