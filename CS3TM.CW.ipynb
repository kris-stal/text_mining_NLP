{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CXj_XvX-XiTFLsBe1_4V-M3vQYIPc_N8","timestamp":1743189787592},{"file_id":"1P-rVbBIU8McBJwvuWbMJjmRgaYLCZG48","timestamp":1634806079763},{"file_id":"15ZvXDbGU32-d3afddksL6uCbAarx-gl3","timestamp":1634806036046},{"file_id":"1I3g5DeuhS0sSOBd4DeQhR7OwjaSSx6gy","timestamp":1615032664939},{"file_id":"1lTmqMvgIU4LIna3kklDa8J7UncnycGCy","timestamp":1615030669644}],"collapsed_sections":["934uDa63sRnp","El_vU9NocxVC"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["**Prior study**:\n","\n","Please use this page as companion to understand the newsgroup data set.\n","[Data Set](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n","You will also need to be familiar with some text processing commands：\n","\n","[Tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n","\n","[countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"],"metadata":{"id":"1ZVVMai8xRFX"}},{"cell_type":"code","metadata":{"id":"yhgYu18FM-_L","executionInfo":{"status":"ok","timestamp":1748269423049,"user_tz":-60,"elapsed":40,"user":{"displayName":"kris","userId":"01768602552210981653"}}},"source":["from IPython import get_ipython\n","get_ipython().magic('reset -sf')"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QXdrc29D71MH"},"source":["# **Steps outline**\n","1. Download your data set by inputting your student number.\n","2. Process your text data, extract features, convert them into vectors\n","3. Modeling, train models on the data set (select model, tune different parameters)\n","4. Process your text data, extract features, convert them into vectors\n","5. Analysis and discussions"]},{"cell_type":"markdown","metadata":{"id":"934uDa63sRnp"},"source":["# Step 1: Load Dataset\n"]},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"2k-V81t0z_NF"}},{"cell_type":"code","metadata":{"id":"cxDjTTcDrtVl","executionInfo":{"status":"ok","timestamp":1748269423697,"user_tz":-60,"elapsed":647,"user":{"displayName":"kris","userId":"01768602552210981653"}}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n","from sklearn.datasets import fetch_20newsgroups\n","twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n","twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8VrHduYC-kd"},"source":["**This is how to identify which data set to use (Please copy  the following information in report front   page).**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvU6i2KNHzC4","executionInfo":{"status":"ok","timestamp":1748269428660,"user_tz":-60,"elapsed":4956,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"86c7a472-d538-4cf4-b9a2-430eff047d86"},"source":["index=input('type your student number?')"],"execution_count":69,"outputs":[{"name":"stdout","output_type":"stream","text":["type your student number?310117329\n"]}]},{"cell_type":"code","source":["x=divmod(int(index),4)\n","yourdata1=x[1]\n","y=divmod(int(index),3)\n","yourdata2=y[1]\n","\n","print('This is your data set index ----> (', x[1], y[1], ')' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8Gmf-HdGcWV","executionInfo":{"status":"ok","timestamp":1748269428675,"user_tz":-60,"elapsed":14,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"d8f1a62f-0603-4aaa-c738-2c82748a0d4a"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["This is your data set index ----> ( 1 0 )\n"]}]},{"cell_type":"markdown","source":["**NOTE: If your two data sets indices are the same, please add your student number a small number, try again.**"],"metadata":{"id":"dRpS0colS0pR"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXPpVRSGAPM7","executionInfo":{"status":"ok","timestamp":1748269428680,"user_tz":-60,"elapsed":4,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"99d9fd60-d678-48da-f5b8-d7056f86b916"},"source":["data1= twenty_train.target_names[x[1]]\n","data2= twenty_train.target_names[y[1]]\n","categories1=[data1,data2]\n","print(categories1)"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["['comp.graphics', 'alt.atheism']\n"]}]},{"cell_type":"markdown","metadata":{"id":"aNjBtO7-DOsu"},"source":["**Your front page data information Ends here**"]},{"cell_type":"markdown","metadata":{"id":"v1jAHpjtaSPu"},"source":["# Step 2 Process your text data, extract features"]},{"cell_type":"markdown","metadata":{"id":"El_vU9NocxVC"},"source":["# 2.1 An example of preprocessing"]},{"cell_type":"markdown","metadata":{"id":"eDHwDyKzNirS"},"source":["**An example is provided.**"]},{"cell_type":"markdown","source":["Please pay attention  comment #replace ..., which means you need to change example text to your data set.\n","Use google search for usages of  \"nltk tokenizer ”, \"nltk stemmer\", \"nltk pos tag\" to help your report writing."],"metadata":{"id":"utJ6ap1bacK3"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wC3yT07PJnKp","executionInfo":{"status":"ok","timestamp":1748269428870,"user_tz":-60,"elapsed":183,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"0d79103d-59df-49fd-c8a2-2b49dac9c744"},"source":["# write your own NLP precessing examples with  preprocessing techniques.\n","\n","dataset=twenty_train.data[20]\n","print(dataset)\n","# please   replace 1 in bracket to other data sample and explore the code\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')"],"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["From: dotsonm@dmapub.dma.org (Mark Dotson)\n","Subject: Re: Hell_2:  Black Sabbath\n","Organization: Dayton Microcomputer Association; Dayton, Ohio\n","Lines: 10\n","\n",": I may be wrong, but wasn't Jeff Fenholt part of Black Sabbath?  He's a\n",": MAJOR brother in Christ now.  He totally changed his life around, and\n",": he and his wife go on tours singing, witnessing, and spreading the\n",": gospel for Christ.  I may be wrong about Black Sabbath, but I know he\n",": was in a similar band if it wasn't that particular group...\n","\n","   Yes, but Jeff also speaks out against listening to bands like Black\n","Sabbath. He says they're into all sorts of satanic stuff. I don't know.\n","\n","                          Mark (dotsonm@dmapub.dma.org)\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748269428882,"user_tz":-60,"elapsed":6,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"858c3e54-2f9f-4d7d-e028-f4c84b191e66","id":"cRHBOWMnAAWA"},"source":["# tokenize: search: nltk tokenize\n","example = \"This is an example sentence.\"\n","\n","from nltk.tokenize import word_tokenize\n","example_tokenize =word_tokenize(example)\n","example_tokenize= word_tokenize(dataset) # replace example in bracket to dataset.\n","print(\"-------------------------tokenize:\")\n","print(example_tokenize)\n"],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------tokenize:\n","['From', ':', 'dotsonm', '@', 'dmapub.dma.org', '(', 'Mark', 'Dotson', ')', 'Subject', ':', 'Re', ':', 'Hell_2', ':', 'Black', 'Sabbath', 'Organization', ':', 'Dayton', 'Microcomputer', 'Association', ';', 'Dayton', ',', 'Ohio', 'Lines', ':', '10', ':', 'I', 'may', 'be', 'wrong', ',', 'but', 'was', \"n't\", 'Jeff', 'Fenholt', 'part', 'of', 'Black', 'Sabbath', '?', 'He', \"'s\", 'a', ':', 'MAJOR', 'brother', 'in', 'Christ', 'now', '.', 'He', 'totally', 'changed', 'his', 'life', 'around', ',', 'and', ':', 'he', 'and', 'his', 'wife', 'go', 'on', 'tours', 'singing', ',', 'witnessing', ',', 'and', 'spreading', 'the', ':', 'gospel', 'for', 'Christ', '.', 'I', 'may', 'be', 'wrong', 'about', 'Black', 'Sabbath', ',', 'but', 'I', 'know', 'he', ':', 'was', 'in', 'a', 'similar', 'band', 'if', 'it', 'was', \"n't\", 'that', 'particular', 'group', '...', 'Yes', ',', 'but', 'Jeff', 'also', 'speaks', 'out', 'against', 'listening', 'to', 'bands', 'like', 'Black', 'Sabbath', '.', 'He', 'says', 'they', \"'re\", 'into', 'all', 'sorts', 'of', 'satanic', 'stuff', '.', 'I', 'do', \"n't\", 'know', '.', 'Mark', '(', 'dotsonm', '@', 'dmapub.dma.org', ')']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748269428890,"user_tz":-60,"elapsed":7,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"646ec18a-c3f7-4644-a91f-178f6666ddc5","id":"kAaaP86_Ahmo"},"source":["# stemmer: search: nltk stemmer\n","stemmer = nltk.stem.PorterStemmer()\n","example_stem = stemmer.stem(dataset)  # replace .....\n","print(\"-------------------------stem:\")\n","print(example_stem)"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------stem:\n","from: dotsonm@dmapub.dma.org (mark dotson)\n","subject: re: hell_2:  black sabbath\n","organization: dayton microcomputer association; dayton, ohio\n","lines: 10\n","\n",": i may be wrong, but wasn't jeff fenholt part of black sabbath?  he's a\n",": major brother in christ now.  he totally changed his life around, and\n",": he and his wife go on tours singing, witnessing, and spreading the\n",": gospel for christ.  i may be wrong about black sabbath, but i know he\n",": was in a similar band if it wasn't that particular group...\n","\n","   yes, but jeff also speaks out against listening to bands like black\n","sabbath. he says they're into all sorts of satanic stuff. i don't know.\n","\n","                          mark (dotsonm@dmapub.dma.org)\n","\n"]}]},{"cell_type":"code","source":["# pos_taging: search: nltk pos tagging example\n","example_posTag=nltk.pos_tag(example_tokenize)\n","print(\"-------------------------pos_taging:\")\n","print(example_posTag)"],"metadata":{"id":"iL4Vr-m0ApLu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748269428947,"user_tz":-60,"elapsed":55,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"0e5d53d6-8aee-440f-9c49-360366016173"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------pos_taging:\n","[('From', 'IN'), (':', ':'), ('dotsonm', 'NN'), ('@', 'NN'), ('dmapub.dma.org', 'NN'), ('(', '('), ('Mark', 'NNP'), ('Dotson', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('Re', 'NN'), (':', ':'), ('Hell_2', 'NN'), (':', ':'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('Organization', 'NNP'), (':', ':'), ('Dayton', 'NNP'), ('Microcomputer', 'NNP'), ('Association', 'NNP'), (';', ':'), ('Dayton', 'NNP'), (',', ','), ('Ohio', 'NNP'), ('Lines', 'NNPS'), (':', ':'), ('10', 'CD'), (':', ':'), ('I', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('wrong', 'JJ'), (',', ','), ('but', 'CC'), ('was', 'VBD'), (\"n't\", 'RB'), ('Jeff', 'NNP'), ('Fenholt', 'NNP'), ('part', 'NN'), ('of', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('?', '.'), ('He', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), (':', ':'), ('MAJOR', 'NN'), ('brother', 'NN'), ('in', 'IN'), ('Christ', 'NNP'), ('now', 'RB'), ('.', '.'), ('He', 'PRP'), ('totally', 'RB'), ('changed', 'VBD'), ('his', 'PRP$'), ('life', 'NN'), ('around', 'RB'), (',', ','), ('and', 'CC'), (':', ':'), ('he', 'PRP'), ('and', 'CC'), ('his', 'PRP$'), ('wife', 'NN'), ('go', 'VBP'), ('on', 'IN'), ('tours', 'NNS'), ('singing', 'VBG'), (',', ','), ('witnessing', 'VBG'), (',', ','), ('and', 'CC'), ('spreading', 'VBG'), ('the', 'DT'), (':', ':'), ('gospel', 'NN'), ('for', 'IN'), ('Christ', 'NNP'), ('.', '.'), ('I', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('wrong', 'JJ'), ('about', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('know', 'VBP'), ('he', 'PRP'), (':', ':'), ('was', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('similar', 'JJ'), ('band', 'NN'), ('if', 'IN'), ('it', 'PRP'), ('was', 'VBD'), (\"n't\", 'RB'), ('that', 'IN'), ('particular', 'JJ'), ('group', 'NN'), ('...', ':'), ('Yes', 'UH'), (',', ','), ('but', 'CC'), ('Jeff', 'NNP'), ('also', 'RB'), ('speaks', 'VBZ'), ('out', 'RP'), ('against', 'IN'), ('listening', 'VBG'), ('to', 'TO'), ('bands', 'NNS'), ('like', 'IN'), ('Black', 'NNP'), ('Sabbath', 'NNP'), ('.', '.'), ('He', 'PRP'), ('says', 'VBZ'), ('they', 'PRP'), (\"'re\", 'VBP'), ('into', 'IN'), ('all', 'DT'), ('sorts', 'NNS'), ('of', 'IN'), ('satanic', 'JJ'), ('stuff', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('.', '.'), ('Mark', 'NNP'), ('(', '('), ('dotsonm', 'JJ'), ('@', 'NNP'), ('dmapub.dma.org', 'NN'), (')', ')')]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748269428953,"user_tz":-60,"elapsed":46,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"282dde99-5399-4fe9-e901-c76d205ad4e4","id":"O-n6AAbc_-_5"},"source":[" # consituency parsing, chunking\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","cp = nltk.RegexpParser(grammar)\n","result = cp.parse(example_posTag)\n","print(result)"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  From/IN\n","  :/:\n","  (NP dotsonm/NN)\n","  (NP @/NN)\n","  (NP dmapub.dma.org/NN)\n","  (/(\n","  Mark/NNP\n","  Dotson/NNP\n","  )/)\n","  (NP Subject/NN)\n","  :/:\n","  (NP Re/NN)\n","  :/:\n","  (NP Hell_2/NN)\n","  :/:\n","  Black/NNP\n","  Sabbath/NNP\n","  Organization/NNP\n","  :/:\n","  Dayton/NNP\n","  Microcomputer/NNP\n","  Association/NNP\n","  ;/:\n","  Dayton/NNP\n","  ,/,\n","  Ohio/NNP\n","  Lines/NNPS\n","  :/:\n","  10/CD\n","  :/:\n","  I/PRP\n","  may/MD\n","  be/VB\n","  wrong/JJ\n","  ,/,\n","  but/CC\n","  was/VBD\n","  n't/RB\n","  Jeff/NNP\n","  Fenholt/NNP\n","  (NP part/NN)\n","  of/IN\n","  Black/NNP\n","  Sabbath/NNP\n","  ?/.\n","  He/PRP\n","  's/VBZ\n","  a/DT\n","  :/:\n","  (NP MAJOR/NN)\n","  (NP brother/NN)\n","  in/IN\n","  Christ/NNP\n","  now/RB\n","  ./.\n","  He/PRP\n","  totally/RB\n","  changed/VBD\n","  his/PRP$\n","  (NP life/NN)\n","  around/RB\n","  ,/,\n","  and/CC\n","  :/:\n","  he/PRP\n","  and/CC\n","  his/PRP$\n","  (NP wife/NN)\n","  go/VBP\n","  on/IN\n","  tours/NNS\n","  singing/VBG\n","  ,/,\n","  witnessing/VBG\n","  ,/,\n","  and/CC\n","  spreading/VBG\n","  the/DT\n","  :/:\n","  (NP gospel/NN)\n","  for/IN\n","  Christ/NNP\n","  ./.\n","  I/PRP\n","  may/MD\n","  be/VB\n","  wrong/JJ\n","  about/IN\n","  Black/NNP\n","  Sabbath/NNP\n","  ,/,\n","  but/CC\n","  I/PRP\n","  know/VBP\n","  he/PRP\n","  :/:\n","  was/VBD\n","  in/IN\n","  (NP a/DT similar/JJ band/NN)\n","  if/IN\n","  it/PRP\n","  was/VBD\n","  n't/RB\n","  that/IN\n","  (NP particular/JJ group/NN)\n","  .../:\n","  Yes/UH\n","  ,/,\n","  but/CC\n","  Jeff/NNP\n","  also/RB\n","  speaks/VBZ\n","  out/RP\n","  against/IN\n","  listening/VBG\n","  to/TO\n","  bands/NNS\n","  like/IN\n","  Black/NNP\n","  Sabbath/NNP\n","  ./.\n","  He/PRP\n","  says/VBZ\n","  they/PRP\n","  're/VBP\n","  into/IN\n","  all/DT\n","  sorts/NNS\n","  of/IN\n","  (NP satanic/JJ stuff/NN)\n","  ./.\n","  I/PRP\n","  do/VBP\n","  n't/RB\n","  know/VB\n","  ./.\n","  Mark/NNP\n","  (/(\n","  dotsonm/JJ\n","  @/NNP\n","  (NP dmapub.dma.org/NN)\n","  )/))\n"]}]},{"cell_type":"markdown","metadata":{"id":"c7VpnVNpKuUt"},"source":["#2.2 NLP Preprocesssing"]},{"cell_type":"markdown","metadata":{"id":"0hmXyhqwOaUQ"},"source":["**Some preprocessing are provided for convenience. Please include why NLP preprocessing is in your report. Explain what techniques have been experimented in your report.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8u5y9adK3tc","executionInfo":{"status":"ok","timestamp":1748269428973,"user_tz":-60,"elapsed":12,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"e0b28a2a-fbe4-4891-d0d2-88d9259d4565"},"source":["import nltk\n","import re\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","stopwordEn = stopwords.words('english')\n","from nltk.corpus import wordnet\n","nltk.download('wordnet')\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n","\n","def lemmaWord(word):\n","    lemma = wordnet.morphy(word)\n","    if lemma is not None:\n","        return lemma\n","    else:\n","        return word\n","\n","def stemWord(word):\n","    stem = stemmer.stem(word)\n","    if stem is not None:\n","        return stem\n","    else:\n","        return word\n","\n","# Base preprocessing method\n","def processText(text, lemma=False, stem=False, gram=1, rmStop=True): # default no lemma or stem, unigram and remove stop words\n","    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE)\n","    # The regex @\\w+ matches an @ followed by word characters, not for full email addresses\n","    tokens = word_tokenize(text)\n","    whitelist = [\"n't\", \"not\", \"no\"]\n","    new_tokens = []\n","    stoplist = stopwordEn if rmStop else []\n","    for i in tokens:\n","        i = i.lower()\n","        if i.isalpha() and (i not in stoplist or i in whitelist):\n","            if lemma:\n","                i = lemmaWord(i)\n","            if stem:\n","                i = stemWord(i)\n","            new_tokens.append(i)\n","    del tokens\n","    if gram <= 1:\n","        return new_tokens\n","    else:\n","        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n","\n","# Further preprocessing method to remove email headers and addresses\n","def cleanText(text):\n","    text = re.sub(r'^(From|Subject|Organization|Lines|Reply-To|NNTP-Posting-Host):.*$', '', text, flags=re.MULTILINE) # Remove headers (From:, Subject:)\n","    text = re.sub(r'\\S+@\\S+', '', text) # Remove full email addresses\n","    return text\n","\n","# Get POS tags method\n","def getTags(text):\n","  token = word_tokenize(text)\n","  token = [l.lower() for l in token]\n","  train_tags = nltk.pos_tag(token)\n","  return [i[1] for i in train_tags]\n"],"execution_count":77,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8mwYOcFcS02","executionInfo":{"status":"ok","timestamp":1748269428982,"user_tz":-60,"elapsed":8,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"34423a12-6509-42ef-c166-d4c3c877ea9b"},"source":["print(processText(dataset))"],"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["['mark', 'dotson', 'subject', 'black', 'sabbath', 'organization', 'dayton', 'microcomputer', 'association', 'dayton', 'ohio', 'lines', 'may', 'wrong', 'jeff', 'fenholt', 'part', 'black', 'sabbath', 'major', 'brother', 'christ', 'totally', 'changed', 'life', 'around', 'wife', 'go', 'tours', 'singing', 'witnessing', 'spreading', 'gospel', 'christ', 'may', 'wrong', 'black', 'sabbath', 'know', 'similar', 'band', 'particular', 'group', 'yes', 'jeff', 'also', 'speaks', 'listening', 'bands', 'like', 'black', 'sabbath', 'says', 'sorts', 'satanic', 'stuff', 'know', 'mark']\n"]}]},{"cell_type":"markdown","metadata":{"id":"44xTvpLa_UC9"},"source":["# Step 3: Build a Pipeline"]},{"cell_type":"markdown","metadata":{"id":"7g5g93owSogu"},"source":["[link text](https://)**Modify the block code below to your choice of classifier [link text](https://www.nltk.org/book/ch06.html):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9HMKvgGMHPB","executionInfo":{"status":"ok","timestamp":1748269429567,"user_tz":-60,"elapsed":584,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"36ccaf74-e897-4e9c-c202-82596fd9d93e"},"source":["# Show before/after cleaning text data\n","\n","print(\"Original text:\\n\", fetch_20newsgroups(subset='train', categories=categories1).data[0][:500])\n","print(\"Cleaned text:\\n\", cleanText(fetch_20newsgroups(subset='train', categories=categories1).data[0])[:500])"],"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n"," From: frank@D012S658.uucp (Frank O'Dwyer)\n","Subject: Re: After 2000 years, can we say that Christian Morality is\n","Organization: Siemens-Nixdorf AG\n","Lines: 28\n","NNTP-Posting-Host: d012s658.ap.mchp.sni.de\n","\n","In article <1993Apr15.125245.12872@abo.fi> MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka) writes:\n","|In <1qie61$fkt@horus.ap.mchp.sni.de> frank@D012S658.uucp writes:\n","|> In article <30114@ursa.bear.com> halat@pooh.bears (Jim Halat) writes:\n","|\n","|> #I'm one of those people who does not know what the word objectiv\n","Cleaned text:\n"," \n","\n","\n","\n","\n","\n","In article   (Mats Andtbacka) writes:\n","|In   writes:\n","|> In article   (Jim Halat) writes:\n","|\n","|> #I'm one of those people who does not know what the word objective means \n","|> #when put next to the word morality.  I assume its an idiom and cannot\n","|> #be defined by its separate terms.\n","|> #\n","|> #Give it a try.\n","|> \n","|> Objective morality is morality built from objective values.\n","|\n","|      \"And these objective values are ... ?\"\n","|Please be specific, and more importantly, motivate.\n","\n","I'll take a wild guess\n"]}]},{"cell_type":"markdown","source":["Without modification, the code will output all four classes.\n","\n","\n","I included some commented codes in places where you may use to change to two class data sets   from your student number, and use logistic model.\n","Your data sets can be obtained as twenty_train1, twenty_test1. All  data set names can be adjusted to get it right."],"metadata":{"id":"Yl4relrvUjHt"}},{"cell_type":"code","metadata":{"id":"9PDFkEEiL1GQ","executionInfo":{"status":"ok","timestamp":1748269430369,"user_tz":-60,"elapsed":801,"user":{"displayName":"kris","userId":"01768602552210981653"}}},"source":["twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)\n","twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)\n","\n","\n","raw_train_data = twenty_train1.data  # RAW text data - NO cleaning\n","raw_test_data = twenty_test1.data # RAW\n","\n","clean_train_data = [cleanText(doc) for doc in raw_train_data] # CLEANED text data\n","clean_test_data = [cleanText(doc) for doc in raw_test_data] # CLEANED"],"execution_count":80,"outputs":[]},{"cell_type":"code","source":["sample_text = twenty_train1.data[0]\n","\n","# Before processing\n","print(\"Before processing:\\n\", sample_text)\n","\n","# After processing\n","print(\"After processing:\\n\", processText(sample_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qx9DEr9nY6sl","executionInfo":{"status":"ok","timestamp":1748269430432,"user_tz":-60,"elapsed":62,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"ccada15a-1bda-4a49-e10b-41583f2d1feb"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Before processing:\n"," From: frank@D012S658.uucp (Frank O'Dwyer)\n","Subject: Re: After 2000 years, can we say that Christian Morality is\n","Organization: Siemens-Nixdorf AG\n","Lines: 28\n","NNTP-Posting-Host: d012s658.ap.mchp.sni.de\n","\n","In article <1993Apr15.125245.12872@abo.fi> MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka) writes:\n","|In <1qie61$fkt@horus.ap.mchp.sni.de> frank@D012S658.uucp writes:\n","|> In article <30114@ursa.bear.com> halat@pooh.bears (Jim Halat) writes:\n","|\n","|> #I'm one of those people who does not know what the word objective means \n","|> #when put next to the word morality.  I assume its an idiom and cannot\n","|> #be defined by its separate terms.\n","|> #\n","|> #Give it a try.\n","|> \n","|> Objective morality is morality built from objective values.\n","|\n","|      \"And these objective values are ... ?\"\n","|Please be specific, and more importantly, motivate.\n","\n","I'll take a wild guess and say Freedom is objectively valuable.  I base\n","this on the assumption that if everyone in the world were deprived utterly\n","of their freedom (so that their every act was contrary to their volition),\n","almost all would want to complain.  Therefore I take it that to assert or\n","believe that \"Freedom is not very valuable\", when almost everyone can see\n","that it is, is every bit as absurd as to assert \"it is not raining\" on\n","a rainy day.  I take this to be a candidate for an objective value, and it\n","it is a necessary condition for objective morality that objective values\n","such as this exist.\n","\n","-- \n","Frank O'Dwyer                                  'I'm not hatching That'\n","odwyer@sse.ie                                  from \"Hens\",  by Evelyn Conlon\n","\n","After processing:\n"," ['frank', 'subject', 'years', 'say', 'christian', 'morality', 'organization', 'ag', 'lines', 'article', 'mats', 'andtbacka', 'writes', 'writes', 'article', 'jim', 'halat', 'writes', 'one', 'people', 'not', 'know', 'word', 'objective', 'means', 'put', 'next', 'word', 'morality', 'assume', 'idiom', 'not', 'defined', 'separate', 'terms', 'give', 'try', 'objective', 'morality', 'morality', 'built', 'objective', 'values', 'objective', 'values', 'specific', 'importantly', 'motivate', 'take', 'wild', 'guess', 'say', 'freedom', 'objectively', 'valuable', 'base', 'assumption', 'everyone', 'world', 'deprived', 'utterly', 'freedom', 'every', 'act', 'contrary', 'volition', 'almost', 'would', 'want', 'complain', 'therefore', 'take', 'assert', 'believe', 'freedom', 'not', 'valuable', 'almost', 'everyone', 'see', 'every', 'bit', 'absurd', 'assert', 'not', 'raining', 'rainy', 'day', 'take', 'candidate', 'objective', 'value', 'necessary', 'condition', 'objective', 'morality', 'objective', 'values', 'exist', 'frank', 'not', 'hatching', 'hens', 'evelyn', 'conlon']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZRqySa8cbr1","executionInfo":{"status":"ok","timestamp":1748269430433,"user_tz":-60,"elapsed":5,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"828c025e-1c61-4a76-9b08-6cd51e52d14f"},"source":["print(\"POS tags:\\n\", getTags(twenty_train1.data[0]))"],"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["POS tags:\n"," ['IN', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'NN', ':', 'NN', ':', 'IN', 'CD', 'NNS', ',', 'MD', 'PRP', 'VB', 'IN', 'JJ', 'NN', 'VBZ', 'NN', ':', 'JJ', 'NN', 'NNS', ':', 'CD', 'NN', ':', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', '(', 'NNS', 'VBP', ')', 'VBZ', ':', 'NN', 'VBZ', 'CD', '$', 'JJ', 'NNP', 'NN', 'NNP', 'VBD', 'NNP', 'NN', 'NNS', ':', 'NN', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'JJ', 'NNP', 'NN', 'NN', 'NNS', '(', 'JJ', 'NN', ')', 'VBZ', ':', 'NN', 'NNP', 'NNP', '#', 'NN', 'VBP', 'CD', 'IN', 'DT', 'NNS', 'WP', 'VBZ', 'RB', 'VB', 'WP', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NNP', '#', 'WRB', 'VBN', 'RB', 'TO', 'DT', 'NN', 'NN', '.', 'JJ', 'VBP', 'PRP$', 'DT', 'NN', 'CC', 'MD', 'RB', 'VB', 'JJ', '#', 'VB', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', '.', 'NN', 'JJ', '#', 'NNP', 'NNP', '#', 'VB', 'PRP', 'DT', 'NN', '.', 'JJ', 'JJ', 'NN', 'NNP', 'JJ', 'NN', 'VBZ', 'JJ', 'VBN', 'IN', 'JJ', 'NNS', '.', 'VB', 'JJ', '``', 'CC', 'DT', 'JJ', 'NNS', 'VBP', ':', '.', \"''\", 'NN', 'VB', 'JJ', ',', 'CC', 'RBR', 'RB', ',', 'NN', '.', 'JJ', 'MD', 'VB', 'DT', 'JJ', 'NN', 'CC', 'VB', 'NN', 'VBZ', 'RB', 'JJ', '.', 'JJ', 'NN', 'DT', 'IN', 'DT', 'NN', 'IN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '(', 'IN', 'DT', 'PRP$', 'DT', 'NN', 'VBD', 'JJ', 'TO', 'PRP$', 'NN', ')', ',', 'RB', 'DT', 'MD', 'VB', 'TO', 'VB', '.', 'NN', 'JJ', 'VBP', 'PRP', 'IN', 'TO', 'VB', 'CC', 'VB', 'IN', '``', 'NN', 'VBZ', 'RB', 'RB', 'JJ', \"''\", ',', 'WRB', 'RB', 'NN', 'MD', 'VB', 'IN', 'PRP', 'VBZ', ',', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'IN', 'TO', 'VB', '``', 'PRP', 'VBZ', 'RB', 'VBG', \"''\", 'IN', 'DT', 'JJ', 'NN', '.', 'VB', 'VBP', 'DT', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'WDT', 'JJ', 'NNS', 'JJ', 'IN', 'DT', 'NN', '.', ':', 'JJ', 'NN', 'POS', 'NN', 'VBP', 'RB', 'VBG', 'IN', \"''\", 'JJ', 'NNP', 'NN', 'IN', '``', 'NNS', \"''\", ',', 'IN', 'NN', 'NN']\n"]}]},{"cell_type":"code","source":["# Show tokens for document after processing\n","print(processText(twenty_train1.data[0]))\n","\n","# Show POS tags for a document\n","print(getTags(twenty_train1.data[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POlF_eC0QFuR","executionInfo":{"status":"ok","timestamp":1748269430436,"user_tz":-60,"elapsed":5,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"dc2beb12-c71f-4c80-8a8a-b07ba42ea8a0"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["['frank', 'subject', 'years', 'say', 'christian', 'morality', 'organization', 'ag', 'lines', 'article', 'mats', 'andtbacka', 'writes', 'writes', 'article', 'jim', 'halat', 'writes', 'one', 'people', 'not', 'know', 'word', 'objective', 'means', 'put', 'next', 'word', 'morality', 'assume', 'idiom', 'not', 'defined', 'separate', 'terms', 'give', 'try', 'objective', 'morality', 'morality', 'built', 'objective', 'values', 'objective', 'values', 'specific', 'importantly', 'motivate', 'take', 'wild', 'guess', 'say', 'freedom', 'objectively', 'valuable', 'base', 'assumption', 'everyone', 'world', 'deprived', 'utterly', 'freedom', 'every', 'act', 'contrary', 'volition', 'almost', 'would', 'want', 'complain', 'therefore', 'take', 'assert', 'believe', 'freedom', 'not', 'valuable', 'almost', 'everyone', 'see', 'every', 'bit', 'absurd', 'assert', 'not', 'raining', 'rainy', 'day', 'take', 'candidate', 'objective', 'value', 'necessary', 'condition', 'objective', 'morality', 'objective', 'values', 'exist', 'frank', 'not', 'hatching', 'hens', 'evelyn', 'conlon']\n","['IN', ':', 'JJ', 'NN', 'NN', '(', 'JJ', 'NN', ')', 'NN', ':', 'NN', ':', 'IN', 'CD', 'NNS', ',', 'MD', 'PRP', 'VB', 'IN', 'JJ', 'NN', 'VBZ', 'NN', ':', 'JJ', 'NN', 'NNS', ':', 'CD', 'NN', ':', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'NN', 'NNP', 'NN', 'NNP', 'NN', '(', 'NNS', 'VBP', ')', 'VBZ', ':', 'NN', 'VBZ', 'CD', '$', 'JJ', 'NNP', 'NN', 'NNP', 'VBD', 'NNP', 'NN', 'NNS', ':', 'NN', 'NN', 'IN', 'NN', '$', 'CD', 'NNP', 'JJ', 'NNP', 'NN', 'NN', 'NNS', '(', 'JJ', 'NN', ')', 'VBZ', ':', 'NN', 'NNP', 'NNP', '#', 'NN', 'VBP', 'CD', 'IN', 'DT', 'NNS', 'WP', 'VBZ', 'RB', 'VB', 'WP', 'DT', 'NN', 'NN', 'VBZ', 'JJ', 'NNP', '#', 'WRB', 'VBN', 'RB', 'TO', 'DT', 'NN', 'NN', '.', 'JJ', 'VBP', 'PRP$', 'DT', 'NN', 'CC', 'MD', 'RB', 'VB', 'JJ', '#', 'VB', 'VBN', 'IN', 'PRP$', 'JJ', 'NNS', '.', 'NN', 'JJ', '#', 'NNP', 'NNP', '#', 'VB', 'PRP', 'DT', 'NN', '.', 'JJ', 'JJ', 'NN', 'NNP', 'JJ', 'NN', 'VBZ', 'JJ', 'VBN', 'IN', 'JJ', 'NNS', '.', 'VB', 'JJ', '``', 'CC', 'DT', 'JJ', 'NNS', 'VBP', ':', '.', \"''\", 'NN', 'VB', 'JJ', ',', 'CC', 'RBR', 'RB', ',', 'NN', '.', 'JJ', 'MD', 'VB', 'DT', 'JJ', 'NN', 'CC', 'VB', 'NN', 'VBZ', 'RB', 'JJ', '.', 'JJ', 'NN', 'DT', 'IN', 'DT', 'NN', 'IN', 'IN', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', 'RB', 'IN', 'PRP$', 'NN', '(', 'IN', 'DT', 'PRP$', 'DT', 'NN', 'VBD', 'JJ', 'TO', 'PRP$', 'NN', ')', ',', 'RB', 'DT', 'MD', 'VB', 'TO', 'VB', '.', 'NN', 'JJ', 'VBP', 'PRP', 'IN', 'TO', 'VB', 'CC', 'VB', 'IN', '``', 'NN', 'VBZ', 'RB', 'RB', 'JJ', \"''\", ',', 'WRB', 'RB', 'NN', 'MD', 'VB', 'IN', 'PRP', 'VBZ', ',', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'IN', 'TO', 'VB', '``', 'PRP', 'VBZ', 'RB', 'VBG', \"''\", 'IN', 'DT', 'JJ', 'NN', '.', 'VB', 'VBP', 'DT', 'TO', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'CC', 'PRP', 'PRP', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', 'WDT', 'JJ', 'NNS', 'JJ', 'IN', 'DT', 'NN', '.', ':', 'JJ', 'NN', 'POS', 'NN', 'VBP', 'RB', 'VBG', 'IN', \"''\", 'JJ', 'NNP', 'NN', 'IN', '``', 'NNS', \"''\", ',', 'IN', 'NN', 'NN']\n"]}]},{"cell_type":"code","metadata":{"id":"vNm3axlhdzlF","executionInfo":{"status":"ok","timestamp":1748269441136,"user_tz":-60,"elapsed":39,"user":{"displayName":"kris","userId":"01768602552210981653"}}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","\n","# Baseline - no cleaning, no lemma\n","baseline_pipeline = Pipeline([\n","    ('vect', CountVectorizer(analyzer=processText)),  # processText with default lemma=False\n","    ('tfidf', TfidfTransformer()), # TF-IDF Transformer\n","    ('clf', LogisticRegression(max_iter=1000)) # Logistic Regression classifier with max iterations set to 1000 to reach convergence\n","])\n","\n","\n","# With cleanText, no lemma\n","clean_pipeline = Pipeline([\n","    ('vect', CountVectorizer(analyzer=processText)),  # processText with default lemma=False\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","\n","# With cleanText and lemma\n","lemma_pipeline = Pipeline([\n","    ('vect', CountVectorizer(analyzer=lambda x: processText(x, lemma=True))), # processText with lemma=True\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","\n","# With cleanText and stem\n","stem_pipeline = Pipeline([\n","    ('vect', CountVectorizer(analyzer=lambda x: processText(x, stem=True))), # processText with stem=True\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","\n","# With cleanText and lemma with stem\n","lemmastem_pipeline = Pipeline([\n","    ('vect', CountVectorizer(analyzer=lambda x: processText(x, lemma=True, stem=True))),# processText with lemma=True AND stem=True\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"Vuq37Bf3Qjpn","executionInfo":{"status":"ok","timestamp":1748269465952,"user_tz":-60,"elapsed":22916,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"c176db6a-3d4b-4b98-a590-f18eb22699d6"},"source":["# To train the models\n","\n","baseline_pipeline.fit(raw_train_data, twenty_train1.target)\n","clean_pipeline.fit(clean_train_data, twenty_train1.target)\n","lemma_pipeline.fit(clean_train_data, twenty_train1.target)\n","stem_pipeline.fit(clean_train_data, twenty_train1.target)\n","lemmastem_pipeline.fit(clean_train_data, twenty_train1.target)"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('vect',\n","                 CountVectorizer(analyzer=<function <lambda> at 0x7a8678eff380>)),\n","                ('tfidf', TfidfTransformer()),\n","                ('clf', LogisticRegression(max_iter=1000))])"],"text/html":["<style>#sk-container-id-3 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-3 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-3 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-3 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-3 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-3 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-3 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-3 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-3 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-3 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-3 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-3 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-3 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-3 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-3 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-3 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-3 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-3 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-3 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-3 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-3 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-3 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-3 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-3 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n","                 CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)),\n","                (&#x27;tfidf&#x27;, TfidfTransformer()),\n","                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n","                 CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)),\n","                (&#x27;tfidf&#x27;, TfidfTransformer()),\n","                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(analyzer=&lt;function &lt;lambda&gt; at 0x7a8678eff380&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div></div></div>"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"gjQ8DmPNRUuJ"},"source":["# Step 4: Make Prediction"]},{"cell_type":"code","metadata":{"id":"OMdoIHjMRWce","executionInfo":{"status":"ok","timestamp":1748269484418,"user_tz":-60,"elapsed":16503,"user":{"displayName":"kris","userId":"01768602552210981653"}}},"source":["# To make predictions with dev/test set\n","\n","baseline_pred = baseline_pipeline.predict(raw_test_data)\n","clean_pred = clean_pipeline.predict(clean_test_data)\n","lemma_pred = lemma_pipeline.predict(clean_test_data)\n","stem_pred = stem_pipeline.predict(clean_test_data)\n","lemmastem_pred = lemmastem_pipeline.predict(clean_test_data)"],"execution_count":86,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GXHJHqoBmyJ"},"source":["# Step 5: Evaluation"]},{"cell_type":"markdown","source":["**You need to modify the code so only two classes from your student number are output as matrix.**"],"metadata":{"id":"QuBuHl6G5xhN"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdB9js0QDErf","executionInfo":{"status":"ok","timestamp":1748269647651,"user_tz":-60,"elapsed":46,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"52e2e031-ee93-4557-c6d8-8210739c4144"},"source":["from sklearn.metrics import classification_report, accuracy_score, confusion_matrix # get evaluation metrics / error confusion matrix\n","import pandas as pd\n","\n","# get all pipelines\n","pipelines = [\n","    (\"Baseline\", baseline_pred),\n","    (\"With Clean Text\", clean_pred),\n","    (\"With Clean Text + Lemma\", lemma_pred),\n","    (\"With Clean Text + Stem\", stem_pred),\n","    (\"With Clean Text + Lemma + Stem\", lemmastem_pred)\n","]\n","\n","# for each pipeline, perform evaluation\n","for name, preds in pipelines:\n","    print(f\"\\n{name}:\")\n","    print(\"Accuracy:\", accuracy_score(twenty_test1.target, preds))\n","    print(\"Classification Report:\")\n","    print(classification_report(twenty_test1.target, preds, target_names=twenty_test1.target_names))"],"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Baseline:\n","Accuracy: 0.9505649717514124\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","  alt.atheism       0.98      0.91      0.94       319\n","comp.graphics       0.93      0.99      0.96       389\n","\n","     accuracy                           0.95       708\n","    macro avg       0.96      0.95      0.95       708\n"," weighted avg       0.95      0.95      0.95       708\n","\n","\n","With Clean Text:\n","Accuracy: 0.9519774011299436\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","  alt.atheism       0.97      0.92      0.95       319\n","comp.graphics       0.94      0.98      0.96       389\n","\n","     accuracy                           0.95       708\n","    macro avg       0.95      0.95      0.95       708\n"," weighted avg       0.95      0.95      0.95       708\n","\n","\n","With Clean Text + Lemma:\n","Accuracy: 0.9562146892655368\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","  alt.atheism       0.98      0.92      0.95       319\n","comp.graphics       0.94      0.98      0.96       389\n","\n","     accuracy                           0.96       708\n","    macro avg       0.96      0.95      0.96       708\n"," weighted avg       0.96      0.96      0.96       708\n","\n","\n","With Clean Text + Stem:\n","Accuracy: 0.96045197740113\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","  alt.atheism       0.98      0.93      0.95       319\n","comp.graphics       0.94      0.99      0.96       389\n","\n","     accuracy                           0.96       708\n","    macro avg       0.96      0.96      0.96       708\n"," weighted avg       0.96      0.96      0.96       708\n","\n","\n","With Clean Text + Lemma + Stem:\n","Accuracy: 0.96045197740113\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","  alt.atheism       0.98      0.93      0.95       319\n","comp.graphics       0.94      0.99      0.96       389\n","\n","     accuracy                           0.96       708\n","    macro avg       0.96      0.96      0.96       708\n"," weighted avg       0.96      0.96      0.96       708\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"qCLCqFXPQsRq"},"source":["# Step 6: Error Analysis and Discussion\n","write down your own obseration about the predictions. Consider both confusion matrix and selected examples. Which classes are predicted correctly or incorrecly, possible explaination, possible solutions\n","\n","Exmaple: 1) Lab Practical, which feature is helpful for female name classification. https://www.nltk.org/book/ch06.html\n","2) research paper: https://github.com/yoonkim/CNN_sentence\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvBw9qkKDS-m","executionInfo":{"status":"ok","timestamp":1748270229069,"user_tz":-60,"elapsed":46,"user":{"displayName":"kris","userId":"01768602552210981653"}},"outputId":"4859aeee-2b1b-4211-e878-c76fc8f23297"},"source":["# for each pipeline, get misclassified examples (5) and confusion matrix\n","for name, preds in pipelines:\n","    df_pred = pd.DataFrame({\n","        'news': twenty_test1.data,\n","        'prediction': preds,\n","        'true': twenty_test1.target\n","    })\n","\n","    misclassified = df_pred[df_pred['true'] != df_pred['prediction']]\n","    print(f\"\\nMisclassified examples for {name}:\")\n","    print(misclassified.head(5))  # Only first 5 to show all clearly\n","\n","    print(\"Confusion Matrix:\")\n","    print(pd.DataFrame(\n","        confusion_matrix(twenty_test1.target, preds),\n","        columns=twenty_test1.target_names,\n","        index=twenty_test1.target_names\n","    ))"],"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Misclassified examples for Baseline:\n","                                                 news  prediction  true\n","0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n","4   Organization: Penn State University\\nFrom: <SM...           1     0\n","15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n","20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n","82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n","Confusion Matrix:\n","               alt.atheism  comp.graphics\n","alt.atheism            289             30\n","comp.graphics            5            384\n","\n","Misclassified examples for With Clean Text:\n","                                                 news  prediction  true\n","0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n","4   Organization: Penn State University\\nFrom: <SM...           1     0\n","15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n","20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n","82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n","Confusion Matrix:\n","               alt.atheism  comp.graphics\n","alt.atheism            293             26\n","comp.graphics            8            381\n","\n","Misclassified examples for With Clean Text + Lemma:\n","                                                 news  prediction  true\n","0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n","4   Organization: Penn State University\\nFrom: <SM...           1     0\n","15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n","20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n","82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n","Confusion Matrix:\n","               alt.atheism  comp.graphics\n","alt.atheism            295             24\n","comp.graphics            7            382\n","\n","Misclassified examples for With Clean Text + Stem:\n","                                                 news  prediction  true\n","0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n","4   Organization: Penn State University\\nFrom: <SM...           1     0\n","15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n","20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n","82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n","Confusion Matrix:\n","               alt.atheism  comp.graphics\n","alt.atheism            296             23\n","comp.graphics            5            384\n","\n","Misclassified examples for With Clean Text + Lemma + Stem:\n","                                                 news  prediction  true\n","0   From: aaron@minster.york.ac.uk\\nSubject: Re: G...           1     0\n","4   Organization: Penn State University\\nFrom: <SM...           1     0\n","15  From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana...           0     1\n","20  From: kax@cs.nott.ac.uk (Kevin Anthoney)\\nSubj...           1     0\n","82  From: aaron@minster.york.ac.uk\\nSubject: Re: D...           1     0\n","Confusion Matrix:\n","               alt.atheism  comp.graphics\n","alt.atheism            296             23\n","comp.graphics            5            384\n"]}]},{"cell_type":"markdown","metadata":{"id":"DRIRg2rufjPu"},"source":["#References:  \n","\n","\n","https://www.nltk.org/book/ch06.html\n","\n"," https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html\n","\n","Search  online resources:\n","\n","\n","sentiment analysis scikit learn\n","\n","scikit learn or nltk + NLP techniques\n","\n","python + NLP techniques\n","\n","scikit learn logistic regression\n","\n","\n"]}]}